{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KUONTOL KEJEPIT PROJECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import ViTForImageClassification, ViTFeatureExtractor, ViTConfig\n",
    "from torchvision import transforms\n",
    "import os\n",
    "\n",
    "#optional\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOAD DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'load_dataset' from 'datasets' (unknown location)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_dataset, DatasetDict\n\u001b[1;32m      2\u001b[0m dataset \u001b[38;5;241m=\u001b[39m load_dataset(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mashery/chexpert\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'load_dataset' from 'datasets' (unknown location)"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "dataset = load_dataset('ashery/chexpert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset loaded\n"
     ]
    }
   ],
   "source": [
    "# Load train and validation data\n",
    "train_df = pd.read_csv('datasets/CheXpert-v1.0-small/train.csv')\n",
    "valid_df = pd.read_csv('datasets/CheXpert-v1.0-small/valid.csv')\n",
    "\n",
    "if 'train_df' in locals() and 'valid_df' in locals():\n",
    "    print(\"dataset loaded\")\n",
    "else:\n",
    "    print(\"fail load dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Path</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Frontal/Lateral</th>\n",
       "      <th>AP/PA</th>\n",
       "      <th>No Finding</th>\n",
       "      <th>Enlarged Cardiomediastinum</th>\n",
       "      <th>Cardiomegaly</th>\n",
       "      <th>Lung Opacity</th>\n",
       "      <th>Lung Lesion</th>\n",
       "      <th>Edema</th>\n",
       "      <th>Consolidation</th>\n",
       "      <th>Pneumonia</th>\n",
       "      <th>Atelectasis</th>\n",
       "      <th>Pneumothorax</th>\n",
       "      <th>Pleural Effusion</th>\n",
       "      <th>Pleural Other</th>\n",
       "      <th>Fracture</th>\n",
       "      <th>Support Devices</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>223409</th>\n",
       "      <td>CheXpert-v1.0-small/train/patient64537/study2/...</td>\n",
       "      <td>Male</td>\n",
       "      <td>59</td>\n",
       "      <td>Frontal</td>\n",
       "      <td>AP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223410</th>\n",
       "      <td>CheXpert-v1.0-small/train/patient64537/study1/...</td>\n",
       "      <td>Male</td>\n",
       "      <td>59</td>\n",
       "      <td>Frontal</td>\n",
       "      <td>AP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223411</th>\n",
       "      <td>CheXpert-v1.0-small/train/patient64538/study1/...</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>Frontal</td>\n",
       "      <td>AP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223412</th>\n",
       "      <td>CheXpert-v1.0-small/train/patient64539/study1/...</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>Frontal</td>\n",
       "      <td>AP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223413</th>\n",
       "      <td>CheXpert-v1.0-small/train/patient64540/study1/...</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>Frontal</td>\n",
       "      <td>AP</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     Path     Sex  Age  \\\n",
       "223409  CheXpert-v1.0-small/train/patient64537/study2/...    Male   59   \n",
       "223410  CheXpert-v1.0-small/train/patient64537/study1/...    Male   59   \n",
       "223411  CheXpert-v1.0-small/train/patient64538/study1/...  Female    0   \n",
       "223412  CheXpert-v1.0-small/train/patient64539/study1/...  Female    0   \n",
       "223413  CheXpert-v1.0-small/train/patient64540/study1/...  Female    0   \n",
       "\n",
       "       Frontal/Lateral AP/PA  No Finding  Enlarged Cardiomediastinum  \\\n",
       "223409         Frontal    AP         NaN                         NaN   \n",
       "223410         Frontal    AP         NaN                         NaN   \n",
       "223411         Frontal    AP         NaN                         NaN   \n",
       "223412         Frontal    AP         NaN                         NaN   \n",
       "223413         Frontal    AP         1.0                         NaN   \n",
       "\n",
       "        Cardiomegaly  Lung Opacity  Lung Lesion  Edema  Consolidation  \\\n",
       "223409           NaN          -1.0          NaN    NaN            NaN   \n",
       "223410           NaN          -1.0          NaN    NaN            NaN   \n",
       "223411           NaN           NaN          NaN   -1.0            NaN   \n",
       "223412           1.0           1.0          NaN    NaN            NaN   \n",
       "223413           NaN           NaN          NaN    NaN            NaN   \n",
       "\n",
       "        Pneumonia  Atelectasis  Pneumothorax  Pleural Effusion  Pleural Other  \\\n",
       "223409        NaN         -1.0           0.0               1.0            NaN   \n",
       "223410        0.0         -1.0           NaN              -1.0            NaN   \n",
       "223411        NaN          NaN           NaN               NaN            NaN   \n",
       "223412       -1.0          1.0           0.0               NaN            NaN   \n",
       "223413        NaN          NaN           0.0               NaN            NaN   \n",
       "\n",
       "        Fracture  Support Devices  \n",
       "223409       NaN              NaN  \n",
       "223410       NaN              NaN  \n",
       "223411       NaN              NaN  \n",
       "223412       NaN              0.0  \n",
       "223413       NaN              NaN  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display first 5 rows of validation data\n",
    "train_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah data unik pada setiap kolom di train_df (kecuali 'Path'):\n",
      "                            Unique Count  \\\n",
      "Sex                                    3   \n",
      "Age                                   74   \n",
      "Frontal/Lateral                        2   \n",
      "AP/PA                                  4   \n",
      "No Finding                             1   \n",
      "Enlarged Cardiomediastinum             3   \n",
      "Cardiomegaly                           3   \n",
      "Lung Opacity                           3   \n",
      "Lung Lesion                            3   \n",
      "Edema                                  3   \n",
      "Consolidation                          3   \n",
      "Pneumonia                              3   \n",
      "Atelectasis                            3   \n",
      "Pneumothorax                           3   \n",
      "Pleural Effusion                       3   \n",
      "Pleural Other                          3   \n",
      "Fracture                               3   \n",
      "Support Devices                        3   \n",
      "\n",
      "                                                                Unique Values  \n",
      "Sex                                                     Female, Male, Unknown  \n",
      "Age                         68, 87, 83, 41, 20, 33, 42, 69, 81, 76, 50, 22...  \n",
      "Frontal/Lateral                                              Frontal, Lateral  \n",
      "AP/PA                                                     AP, nan, PA, LL, RL  \n",
      "No Finding                                                           1.0, nan  \n",
      "Enlarged Cardiomediastinum                                nan, 0.0, 1.0, -1.0  \n",
      "Cardiomegaly                                              nan, -1.0, 0.0, 1.0  \n",
      "Lung Opacity                                              nan, 1.0, 0.0, -1.0  \n",
      "Lung Lesion                                               nan, 1.0, -1.0, 0.0  \n",
      "Edema                                                     nan, -1.0, 1.0, 0.0  \n",
      "Consolidation                                             nan, -1.0, 0.0, 1.0  \n",
      "Pneumonia                                                 nan, 0.0, -1.0, 1.0  \n",
      "Atelectasis                                               nan, -1.0, 1.0, 0.0  \n",
      "Pneumothorax                                              0.0, nan, 1.0, -1.0  \n",
      "Pleural Effusion                                          nan, -1.0, 0.0, 1.0  \n",
      "Pleural Other                                             nan, 1.0, -1.0, 0.0  \n",
      "Fracture                                                  nan, 1.0, 0.0, -1.0  \n",
      "Support Devices                                           1.0, nan, 0.0, -1.0  \n"
     ]
    }
   ],
   "source": [
    "# Cetak jumlah data unik pada setiap kolom di train_df (kecuali 'Path')\n",
    "print(\"Jumlah data unik pada setiap kolom di train_df (kecuali 'Path'):\")\n",
    "unique_counts = train_df.loc[:, ~train_df.columns.isin(['Path'])].nunique()\n",
    "unique_values = train_df.loc[:, ~train_df.columns.isin(['Path'])].apply(lambda x: [str(val) for val in x.unique()])\n",
    "result = pd.DataFrame({'Unique Count': unique_counts, 'Unique Values': unique_values.apply(lambda x: ', '.join(x))})\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PRE PROSESING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hapus data yang tidak diperlukam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kolom ['Sex', 'Age', 'Frontal/Lateral', 'AP/PA'] yang tidak diperlukan telah dihapus.\n"
     ]
    }
   ],
   "source": [
    "# Hapus kolom yang tidak diperlukan\n",
    "columns_to_drop = ['Sex', \n",
    "                   'Age', \n",
    "                   'Frontal/Lateral', \n",
    "                   'AP/PA',\n",
    "                #    'No Finding'\n",
    "                   ]\n",
    "train_df = train_df.drop(columns=columns_to_drop)\n",
    "valid_df = valid_df.drop(columns=columns_to_drop)\n",
    "\n",
    "print(f\"Kolom {columns_to_drop} yang tidak diperlukan telah dihapus.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contoh train_df:\n",
      "CheXpert-v1.0-small/train/patient00001/study1/view1_frontal.jpg : ['No Finding', 'Support Devices']\n",
      "CheXpert-v1.0-small/train/patient00002/study2/view1_frontal.jpg : ['Lung Opacity', 'Fracture']\n",
      "CheXpert-v1.0-small/train/patient00002/study1/view1_frontal.jpg : ['Lung Opacity', 'Fracture']\n",
      "CheXpert-v1.0-small/train/patient00002/study1/view2_lateral.jpg : ['Lung Opacity', 'Fracture']\n",
      "CheXpert-v1.0-small/train/patient00003/study1/view1_frontal.jpg : ['Edema']\n",
      "\n",
      "Contoh valid_df:\n",
      "CheXpert-v1.0-small/valid/patient64541/study1/view1_frontal.jpg : ['Enlarged Cardiomediastinum', 'Cardiomegaly', 'Lung Opacity']\n",
      "CheXpert-v1.0-small/valid/patient64542/study1/view1_frontal.jpg : ['Support Devices']\n",
      "CheXpert-v1.0-small/valid/patient64542/study1/view2_lateral.jpg : ['Support Devices']\n",
      "CheXpert-v1.0-small/valid/patient64543/study1/view1_frontal.jpg : ['Enlarged Cardiomediastinum', 'Lung Opacity', 'Edema']\n",
      "CheXpert-v1.0-small/valid/patient64544/study1/view1_frontal.jpg : ['No Finding']\n"
     ]
    }
   ],
   "source": [
    "# Membuat dataset baru untuk train_df: path -> [label1, label2, ...] hanya untuk label dengan nilai 1.0\n",
    "label_columns = train_df.columns[1:]  # kolom label (kecuali 'Path')\n",
    "train_df2 = []\n",
    "for idx, row in train_df.iterrows():\n",
    "    labels = [col for col in label_columns if row[col] == 1.0]\n",
    "    train_df2.append({'Path': row['Path'], 'Labels': labels})\n",
    "\n",
    "# Membuat dataset baru untuk valid_df\n",
    "valid_df2 = []\n",
    "for idx, row in valid_df.iterrows():\n",
    "    labels = [col for col in label_columns if row[col] == 1.0]\n",
    "    valid_df2.append({'Path': row['Path'], 'Labels': labels})\n",
    "\n",
    "# Contoh hasil\n",
    "print(\"Contoh train_df:\")\n",
    "for item in train_df2[:5]:\n",
    "    print(f\"{item['Path']} : {item['Labels']}\")\n",
    "\n",
    "print(\"\\nContoh valid_df:\")\n",
    "for item in valid_df2[:5]:\n",
    "    print(f\"{item['Path']} : {item['Labels']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_df dan valid_df telah dihapus dari memory.\n"
     ]
    }
   ],
   "source": [
    "del train_df\n",
    "del valid_df\n",
    "print(\"train_df dan valid_df telah dihapus dari memory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MENYIAPKAN DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Membuat Data Test dari Data Valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_df menjadi validasi dan test set\n"
     ]
    }
   ],
   "source": [
    "# Bagi valid_df menjadi validasi dan test set\n",
    "valid_df2, test_df2 = train_test_split(valid_df2, test_size=0.2, random_state=42)\n",
    "print(\"valid_df menjadi validasi dan test set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah data train: 223414\n",
      "Jumlah data validasi: 187\n",
      "Jumlah data test: 47\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Jumlah data train: {len(train_df2)}\")\n",
    "print(f\"Jumlah data validasi: {len(valid_df2)}\")\n",
    "print(f\"Jumlah data test: {len(test_df2)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ViTForImageClassification(\n",
      "  (vit): ViTModel(\n",
      "    (embeddings): ViTEmbeddings(\n",
      "      (patch_embeddings): ViTPatchEmbeddings(\n",
      "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): ViTEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x ViTLayer(\n",
      "          (attention): ViTAttention(\n",
      "            (attention): ViTSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            )\n",
      "            (output): ViTSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ViTIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): ViTOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "  )\n",
      "  (classifier): Linear(in_features=768, out_features=14, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import ViTForImageClassification, ViTConfig\n",
    "\n",
    "# Konfigurasi model Vision Transformer\n",
    "config = ViTConfig(\n",
    "    image_size=224,  # Ukuran input gambar (224x224)\n",
    "    num_labels=14,   # Jumlah label output (disesuaikan dengan dataset)\n",
    "    hidden_size=768, # Ukuran dimensi hidden layer\n",
    "    num_hidden_layers=12, # Jumlah layer transformer\n",
    "    num_attention_heads=12, # Jumlah head pada multi-head attention\n",
    "    intermediate_size=3072, # Ukuran layer intermediate\n",
    "    patch_size=16,    # Ukuran patch gambar\n",
    "    hidden_dropout_prob=0.1, # Dropout pada hidden layer\n",
    "    attention_probs_dropout_prob=0.1, # Dropout pada attention\n",
    ")\n",
    "\n",
    "# Membuat model Vision Transformer\n",
    "model = ViTForImageClassification(config)\n",
    "\n",
    "# Menampilkan arsitektur model\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/rafihaqul/FC86902B868FE50C/Documents/AMIKOM/skripsi/project_1/.venv/lib/python3.10/site-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n",
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at WinKawaks/vit-small-patch16-224 and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([14]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([1000, 384]) in the checkpoint and torch.Size([14, 384]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-trained ViT model dan feature extractor telah dimuat.\n"
     ]
    }
   ],
   "source": [
    "from transformers import ViTFeatureExtractor\n",
    "\n",
    "# Load pre-trained ViT model and feature extractor\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained('WinKawaks/vit-small-patch16-224')\n",
    "model = ViTForImageClassification.from_pretrained(\n",
    "    'WinKawaks/vit-small-patch16-224',\n",
    "    num_labels=14,\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "\n",
    "print(\"Pre-trained ViT model dan feature extractor telah dimuat.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformasi gambar telah ditambahkan.\n"
     ]
    }
   ],
   "source": [
    "# Definisikan transformasi gambar\n",
    "image_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Ubah ukuran gambar menjadi 224x224\n",
    "    transforms.ToTensor(),          # Konversi gambar menjadi tensor\n",
    "    transforms.Normalize(           # Normalisasi gambar dengan mean dan std\n",
    "        mean=[0.485, 0.456, 0.406], \n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n",
    "\n",
    "print(\"Transformasi gambar telah ditambahkan.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CheXpertDataset(Dataset):\n",
    "     def __init__(self, data_list, label_columns, transform):\n",
    "          self.data_list = data_list\n",
    "          self.label_columns = list(label_columns)\n",
    "          self.transform = transform\n",
    "\n",
    "     def __len__(self):\n",
    "          return len(self.data_list)\n",
    "\n",
    "     def __getitem__(self, idx):\n",
    "          item = self.data_list[idx]\n",
    "          img_path = 'datasets/' + item['Path']\n",
    "          image = Image.open(img_path).convert('RGB')\n",
    "          image = self.transform(image)\n",
    "          # Multi-hot encoding\n",
    "          labels = np.zeros(len(self.label_columns), dtype=np.float32)\n",
    "          for label in item['Labels']:\n",
    "               if label in self.label_columns:\n",
    "                    labels[self.label_columns.index(label)] = 1.0\n",
    "          labels = torch.tensor(labels)\n",
    "          return image, labels\n",
    "\n",
    "# DataLoader\n",
    "train_dataset = CheXpertDataset(train_df2, label_columns, image_transforms)\n",
    "valid_dataset = CheXpertDataset(valid_df2, label_columns, image_transforms)\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=2)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=4, shuffle=False, num_workers=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer & Loss\n",
    "optimizer = optim.AdamW(model.parameters(), lr=2e-5)\n",
    "# Asymmetric Loss implementation\n",
    "class AsymmetricLoss(nn.Module):\n",
    "    def __init__(self, gamma_neg=4, gamma_pos=1, clip=0.05, eps=1e-8):\n",
    "        super(AsymmetricLoss, self).__init__()\n",
    "        self.gamma_neg = gamma_neg\n",
    "        self.gamma_pos = gamma_pos\n",
    "        self.clip = clip\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        x_sigmoid = torch.sigmoid(logits)\n",
    "        xs_pos = x_sigmoid\n",
    "        xs_neg = 1 - x_sigmoid\n",
    "\n",
    "        # Asymmetric Clipping\n",
    "        if self.clip is not None and self.clip > 0:\n",
    "            xs_neg = (xs_neg + self.clip).clamp(max=1)\n",
    "\n",
    "        # Basic CE calculation\n",
    "        loss_pos = targets * torch.log(xs_pos.clamp(min=self.eps))\n",
    "        loss_neg = (1 - targets) * torch.log(xs_neg.clamp(min=self.eps))\n",
    "        loss = loss_pos + loss_neg\n",
    "\n",
    "        # Asymmetric Focusing\n",
    "        if self.gamma_neg > 0 or self.gamma_pos > 0:\n",
    "            pt0 = xs_pos * targets\n",
    "            pt1 = xs_neg * (1 - targets)\n",
    "            pt = pt0 + pt1\n",
    "            one_sided_gamma = self.gamma_pos * targets + self.gamma_neg * (1 - targets)\n",
    "            loss *= (1 - pt) ** one_sided_gamma\n",
    "\n",
    "        return -loss.mean()\n",
    "\n",
    "criterion = AsymmetricLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tuning pre-trained ViT model using train_df2 and valid_df2\n",
    "\n",
    "num_epochs = 3  # Atur sesuai kebutuhan\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, targets in train_loader:\n",
    "        images = images.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images).logits\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "\n",
    "        # Free up unused memory\n",
    "        del images, targets, outputs, loss\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] - Train Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, targets in valid_loader:\n",
    "            images = images.to(device)\n",
    "            targets = targets.to(device)\n",
    "            outputs = model(images).logits\n",
    "            loss = criterion(outputs, targets)\n",
    "            val_loss += loss.item() * images.size(0)\n",
    "            del images, targets, outputs, loss\n",
    "            torch.cuda.empty_cache()\n",
    "    val_loss /= len(valid_loader.dataset)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] - Validation Loss: {val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m      9\u001b[0m running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, targets \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m     11\u001b[0m     images \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     12\u001b[0m     targets \u001b[38;5;241m=\u001b[39m targets\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m/media/rafihaqul/FC86902B868FE50C/Documents/AMIKOM/skripsi/project_1/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:733\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    730\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    731\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    732\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 733\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    734\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    735\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    736\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    737\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    738\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    739\u001b[0m ):\n",
      "File \u001b[0;32m/media/rafihaqul/FC86902B868FE50C/Documents/AMIKOM/skripsi/project_1/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:789\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    787\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    788\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 789\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    790\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    791\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/media/rafihaqul/FC86902B868FE50C/Documents/AMIKOM/skripsi/project_1/.venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/media/rafihaqul/FC86902B868FE50C/Documents/AMIKOM/skripsi/project_1/.venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[14], line 13\u001b[0m, in \u001b[0;36mCheXpertDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     11\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_list[idx]\n\u001b[1;32m     12\u001b[0m img_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdatasets/\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m item[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPath\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 13\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     14\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(image)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Multi-hot encoding\u001b[39;00m\n",
      "File \u001b[0;32m/media/rafihaqul/FC86902B868FE50C/Documents/AMIKOM/skripsi/project_1/.venv/lib/python3.10/site-packages/PIL/Image.py:3505\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3502\u001b[0m     filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mfspath(fp)\n\u001b[1;32m   3504\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename:\n\u001b[0;32m-> 3505\u001b[0m     fp \u001b[38;5;241m=\u001b[39m \u001b[43mbuiltins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3506\u001b[0m     exclusive_fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   3507\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Fine-tuning loop untuk training dataset train_df2\n",
    "\n",
    "num_epochs = 3  # Ubah sesuai kebutuhan\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, targets in train_loader:\n",
    "        images = images.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images).logits\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "\n",
    "        # Free up unused memory\n",
    "        del images, targets, outputs, loss\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] - Loss: {epoch_loss:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
